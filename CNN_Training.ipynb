{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../X_train.npy')\n",
    "Y_train = np.load('../Y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dataset have shape = (7726, 700, 700)\n",
      "Train label has shape = (7726,)\n"
     ]
    }
   ],
   "source": [
    "def normalize_data(data): \n",
    "    for i in data:\n",
    "        i = i / i.max()\n",
    "    return data\n",
    "\n",
    "X_train = normalize_data(X_train)\n",
    "print('Image dataset have shape =', X_train.shape)\n",
    "Y_train = np.array(Y_train)\n",
    "print('Train label has shape =', Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dataset have shape = (7726, 700, 700, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "print('Image dataset have shape =', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2682, 1: 5044}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(Y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    " [transforms.ToTensor(),\n",
    " transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x = torch.stack([torch.Tensor(i) for i in X_train])\n",
    "tensor_y = torch.from_numpy(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = utils.TensorDataset(tensor_x,tensor_y) \n",
    "trainloader = utils.DataLoader(trainset,  batch_size=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55, 700, 700, 1])\n",
      "torch.Size([55, 1, 700, 700])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.size())\n",
    "images = images.permute(0, 3, 1, 2)\n",
    "print(images.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_noise(X_torch):\n",
    "    X = X_torch.cpu().numpy()\n",
    "    vals = len(np.unique(X))\n",
    "    vals = 2 ** np.ceil(np.log2(vals))\n",
    "    noisy = np.random.poisson(X * vals) / float(vals)\n",
    "    return noisy\n",
    "\n",
    "def speckle_noise(X_torch):\n",
    "    X = X_torch.cpu().numpy()\n",
    "    X = X.reshape((X.shape[0], X.shape[1])) \n",
    "    row,col = X.shape\n",
    "    s_vs_p = 0.5\n",
    "    amount = 0.02\n",
    "    out = np.copy(X)\n",
    "    num_salt = np.ceil(amount * X.size * s_vs_p)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "              for i in X.shape]\n",
    "    out[coords] = 1\n",
    "    num_pepper = np.ceil(amount* X.size * (1. - s_vs_p))\n",
    "    coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "              for i in X.shape]\n",
    "    out[coords] = 0\n",
    "    return out.reshape((out.shape[0], out.shape[1],1))\n",
    "\n",
    "def flip_2D(X_torch):\n",
    "    return np.fliplr(X_torch.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_batch(inputs, labels):\n",
    "    x = np.zeros((inputs.size()[0]*3,inputs.size()[1],inputs.size()[2], inputs.size()[3]))\n",
    "    y = np.zeros((labels.size()[0]*3))\n",
    "    for i in range(len(inputs)):\n",
    "        x[3*i] = inputs[i].cpu().numpy()\n",
    "        y[3*i] = labels[i].cpu().numpy()\n",
    "        x[3*i+1] = flip_2D(inputs[i])\n",
    "        y[3*i+1] = labels[i]\n",
    "        if (i%2 == 0):\n",
    "            x[3*i+2] = poisson_noise(inputs[i])\n",
    "            y[3*i+2] = labels[i]\n",
    "        else:\n",
    "            x[3*i+2] = speckle_noise(inputs[i])\n",
    "            y[3*i+2] = labels[i]\n",
    "    return torch.from_numpy(x), torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.conv3 = nn.Conv2d(20, 10, 4, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv4 = nn.Conv2d(10, 20, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv5 = nn.Conv2d(20, 10, 4, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.conv6 = nn.Conv2d(10, 20, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv7 = nn.Conv2d(20, 10, 4, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv8 = nn.Conv2d(10, 20, 3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.conv9 = nn.Conv2d(20, 20, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "        self.fc1 = nn.Linear(20, 10)\n",
    "        self.fc2 = nn.Linear(10, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x.float())))\n",
    "        x = self.pool(F.relu(self.conv2(x.float())))\n",
    "        x = self.pool(F.relu(self.conv3(x.float())))\n",
    "        x = self.pool(F.relu(self.conv4(x.float())))\n",
    "        x = self.pool(F.relu(self.conv5(x.float())))\n",
    "        x = self.pool(F.relu(self.conv6(x.float())))\n",
    "        x = self.pool(F.relu(self.conv7(x.float())))\n",
    "        x = self.pool(F.relu(self.conv8(x.float())))\n",
    "        x = self.pool(F.relu(self.conv9(x.float())))\n",
    "        x = x.view(x.size(0), 20)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5): Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7): Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv9): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (fc2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_losses = []\n",
    "epochs = 1000 \n",
    "print_freq = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dwei/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/Users/dwei/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([165, 20, 1, 1])\n",
      "torch.Size([165, 20])\n",
      "torch.Size([165, 10])\n",
      "torch.Size([165, 20, 1, 1])\n",
      "torch.Size([165, 20])\n",
      "torch.Size([165, 10])\n",
      "[epoch: 0, i:     1] avg mini-batch loss: 0.695\n",
      "torch.Size([165, 20, 1, 1])\n",
      "torch.Size([165, 20])\n",
      "torch.Size([165, 10])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-baf116f50cef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs): \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs, labels = augment_batch(inputs, labels)\n",
    "        \n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, labels.long())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1:\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('../X_test.npy')\n",
    "Y_test = np.load('../Y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = normalize_data(X_test)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x_test = torch.stack([torch.Tensor(i) for i in X_test]) \n",
    "tensor_y_test = torch.from_numpy(Y_test)\n",
    "testset = utils.TensorDataset(tensor_x_test,tensor_y_test)\n",
    "testloader = utils.DataLoader(testset,  batch_size= 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([35, 20, 1, 1])\n",
      "torch.Size([35, 20])\n",
      "torch.Size([35, 10])\n",
      "915\n",
      "Accuracy of the network on the test images: 30 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(total)\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(2))\n",
    "class_total = list(0. for i in range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([35, 20, 1, 1])\n",
      "torch.Size([35, 20])\n",
      "torch.Size([35, 10])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 35 is out of bounds for dimension 0 with size 35",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-234f216dbea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m55\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mclass_correct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mclass_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 35 is out of bounds for dimension 0 with size 35"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(55):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "for i in range(2):\n",
    "    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n",
      "torch.Size([55, 20, 1, 1])\n",
      "torch.Size([55, 20])\n",
      "torch.Size([55, 10])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2e18dbd405b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-72843b8a4ccb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    348\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    349\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 350\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(total)\n",
    "print('Accuracy of the network on the train images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
